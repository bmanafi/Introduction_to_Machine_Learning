{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=200 height=200>\n",
    "<br>\n",
    "<font color=0F5298 size=8>\n",
    "Introduction to Machine Learning <br>\n",
    "<font color=696880 size=5>\n",
    "<!-- <br> -->\n",
    "Computer Engineering Department\n",
    "<br>\n",
    "Sharif University of Technology\n",
    "\n",
    "<font color=696880 size=5>\n",
    "<br>\n",
    "CE 40477 - Fall 2024\n",
    "\n",
    "<font color=GREEN size=5>\n",
    "<br>\n",
    "Mahan Bayhaghi & Nikan Vasei\n",
    "<!-- <br> -->\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    A class representing a node in a decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, gain=None, value=None):\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the Node class.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        feature : int, optional\n",
    "            The index of the feature used for splitting at this node.\n",
    "        threshold : float, optional\n",
    "            The threshold value for splitting the dataset at this node.\n",
    "        left : Node, optional\n",
    "            The left child node resulting from the split.\n",
    "        right : Node, optional\n",
    "            The right child node resulting from the split.\n",
    "        gain : float, optional\n",
    "            The information gain of the split at this node.\n",
    "        value : int or float, optional\n",
    "            If this node is a leaf node, this represents the predicted value.\n",
    "        \"\"\"\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    A decision tree classifier for binary classification problems.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_samples=2, max_depth=2):\n",
    "        \"\"\"\n",
    "        Constructor for DecisionTree class.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        min_samples : int\n",
    "            Minimum number of samples required to split an internal node.\n",
    "        max_depth : int\n",
    "            Maximum depth of the decision tree.\n",
    "        \"\"\"\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def _split_data(self, dataset, feature, threshold):\n",
    "        \"\"\"\n",
    "        Splits the given dataset into two datasets based on the given feature and threshold.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            Input dataset.\n",
    "        feature : int\n",
    "            Index of the feature to be split on.\n",
    "        threshold : float\n",
    "            Threshold value to split the feature on.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        left_dataset : ndarray\n",
    "            Subset of the dataset with values less than or equal to the threshold.\n",
    "        right_dataset : ndarray\n",
    "            Subset of the dataset with values greater than the threshold.\n",
    "        \"\"\"\n",
    "        # Create empty arrays to store the left and right datasets\n",
    "        left_dataset = []\n",
    "        right_dataset = []\n",
    "        \n",
    "        # Loop over each row in the dataset and split based on the given feature and threshold\n",
    "        for row in dataset:\n",
    "            if row[feature] <= threshold:\n",
    "                left_dataset.append(row)\n",
    "            else:\n",
    "                right_dataset.append(row)\n",
    "\n",
    "        # Convert the left and right datasets to numpy arrays and return\n",
    "        left_dataset = np.array(left_dataset)\n",
    "        right_dataset = np.array(right_dataset)\n",
    "        return left_dataset, right_dataset\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Computes the entropy of the given label values.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        y : ndarray\n",
    "            Input label values.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        entropy : float\n",
    "            Entropy of the given label values.\n",
    "        \"\"\"\n",
    "        entropy = 0\n",
    "\n",
    "        # Find the unique label values in y and loop over each value\n",
    "        labels = np.unique(y)\n",
    "        for label in labels:\n",
    "            # Find the examples in y that have the current label\n",
    "            label_examples = y[y == label]\n",
    "            # Calculate the ratio of the current label in y\n",
    "            pl = len(label_examples) / len(y)\n",
    "            # Calculate the entropy using the current label and ratio\n",
    "            entropy += -pl * np.log2(pl)\n",
    "\n",
    "        # Return the final entropy value\n",
    "        return entropy\n",
    "\n",
    "    def _information_gain(self, parent, left, right):\n",
    "        \"\"\"\n",
    "        Computes the information gain from splitting the parent dataset into two datasets.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        parent : ndarray\n",
    "            Input parent dataset.\n",
    "        left : ndarray\n",
    "            Subset of the parent dataset after split on a feature.\n",
    "        right : ndarray\n",
    "            Subset of the parent dataset after split on a feature.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        information_gain : float\n",
    "            Information gain of the split.\n",
    "        \"\"\"\n",
    "        # Set initial information gain to 0\n",
    "        information_gain = 0\n",
    "        # Compute entropy for parent\n",
    "        parent_entropy = self._entropy(parent)\n",
    "        # Calculate weight for left and right nodes\n",
    "        weight_left = len(left) / len(parent)\n",
    "        weight_right= len(right) / len(parent)\n",
    "        # Compute entropy for left and right nodes\n",
    "        entropy_left, entropy_right = self._entropy(left), self._entropy(right)\n",
    "        # Calculate weighted entropy \n",
    "        weighted_entropy = weight_left * entropy_left + weight_right * entropy_right\n",
    "        # Calculate information gain \n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "        return information_gain\n",
    "    \n",
    "    def _best_split(self, dataset, num_features):\n",
    "        \"\"\"\n",
    "        Finds the best split for the given dataset.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            The dataset to split.\n",
    "        num_samples : int\n",
    "            The number of samples in the dataset.\n",
    "        num_features : int\n",
    "            The number of features in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        best_split : dict\n",
    "            A dictionary with the best split feature index, threshold, gain, left, and right datasets.\n",
    "        \"\"\"\n",
    "        # Dictionary to store the best split values\n",
    "        best_split = {'gain': -1}\n",
    "        # Loop over all the features\n",
    "        for feature_index in range(num_features):\n",
    "            # Get the feature at the current feature_index\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            # Get unique values of that feature\n",
    "            thresholds = np.unique(feature_values)\n",
    "            # Loop over all values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # Get left and right datasets\n",
    "                left_dataset, right_dataset = self._split_data(dataset, feature_index, threshold)\n",
    "                # Check if either datasets is empty\n",
    "                if len(left_dataset) and len(right_dataset):\n",
    "                    # Get y values of the parent and left, right nodes\n",
    "                    y, left_y, right_y = dataset[:, -1], left_dataset[:, -1], right_dataset[:, -1]\n",
    "                    # Compute information gain based on the y values\n",
    "                    information_gain = self._information_gain(y, left_y, right_y)\n",
    "                    # Update the best split if conditions are met\n",
    "                    if information_gain > best_split[\"gain\"]:\n",
    "                        best_split[\"feature\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"left_dataset\"] = left_dataset\n",
    "                        best_split[\"right_dataset\"] = right_dataset\n",
    "                        best_split[\"gain\"] = information_gain\n",
    "        return best_split\n",
    "    \n",
    "    def _calculate_leaf_value(self, y):\n",
    "        \"\"\"\n",
    "        Calculates the most occurring value in the given list of y values.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        y : list\n",
    "            The list of y values.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        most_occurring_value : int or float\n",
    "            The most occurring value in the list.\n",
    "        \"\"\"\n",
    "        y = list(y)\n",
    "        # Get the highest present class in the array\n",
    "        most_occurring_value = max(y, key=y.count)\n",
    "        return most_occurring_value\n",
    "    \n",
    "    def _build_tree(self, dataset, current_depth=0):\n",
    "        \"\"\"\n",
    "        Recursively builds a decision tree from the given dataset.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        dataset : ndarray\n",
    "            The dataset to build the tree from.\n",
    "        current_depth : int\n",
    "            The current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        Node : Node\n",
    "            The root node of the built decision tree.\n",
    "        \"\"\"\n",
    "        # Split the dataset into X, y values\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        n_samples, n_features = X.shape\n",
    "        # Keeps splitting until stopping conditions are met\n",
    "        if n_samples >= self.min_samples and current_depth <= self.max_depth:\n",
    "            # Get the best split\n",
    "            best_split = self._best_split(dataset, n_features)\n",
    "            # Check if the gain isn't zero\n",
    "            if best_split[\"gain\"]:\n",
    "                # Continue splitting the left and the right child. Increment current depth\n",
    "                left_node = self._build_tree(best_split[\"left_dataset\"], current_depth + 1)\n",
    "                right_node = self._build_tree(best_split[\"right_dataset\"], current_depth + 1)\n",
    "                # Return decision node\n",
    "                return Node(best_split[\"feature\"], best_split[\"threshold\"], left_node, right_node, best_split[\"gain\"])\n",
    "\n",
    "        # Compute the leaf node value\n",
    "        leaf_value = self._calculate_leaf_value(y)\n",
    "        # Return the leaf node value\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Builds and fits the decision tree to the given X and y values.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            The feature matrix.\n",
    "        y : ndarray\n",
    "            The target values.\n",
    "        \"\"\"\n",
    "        dataset = np.concatenate((X, y), axis=1)  \n",
    "        self.root = self._build_tree(dataset)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the class labels for each instance in the feature matrix X.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : ndarray\n",
    "            The feature matrix to make predictions for.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        predictions : list\n",
    "            A list of predicted class labels.\n",
    "        \"\"\"\n",
    "        # Create an empty list to store the predictions\n",
    "        predictions = []\n",
    "        # For each instance in X, make a prediction by traversing the tree\n",
    "        for x in X:\n",
    "            prediction = self._make_prediction(x, self.root)\n",
    "            # Append the prediction to the list of predictions\n",
    "            predictions.append(prediction)\n",
    "        # Convert the list to a numpy array and return it\n",
    "        np.array(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def _make_prediction(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverses the decision tree to predict the target value for the given feature vector.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        x : ndarray\n",
    "            The feature vector to predict the target value for.\n",
    "        node : Node\n",
    "            The current node being evaluated.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        prediction : int or float\n",
    "            The predicted target value for the given feature vector.\n",
    "        \"\"\"\n",
    "        # If the node has value i.e it's a leaf node extract it's value\n",
    "        if node.value != None: \n",
    "            return node.value\n",
    "        else:\n",
    "            # If it's node a leaf node we'll get it's feature and traverse through the tree accordingly\n",
    "            feature = x[node.feature]\n",
    "            if feature <= node.threshold:\n",
    "                return self._make_prediction(x, node.left)\n",
    "            else:\n",
    "                return self._make_prediction(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll use the `Breast Cancer Wisconsin` dataset which is a binary classification dataset, and has 30 different continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also standardize the data using the `StandardScaler` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "y = y.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (455, 30), y_train: (455, 1)\n",
      "Shape of X_test: (455, 30), y_test: (114, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Shape of X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_train.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last we can train a `DecisionTree()` model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Model's Accuracy: 0.9473684210526315\n",
      "DT Model's F1-Score: 0.9583333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Create a DT model.\n",
    "dt_model = DecisionTree(2, 2)\n",
    "\n",
    "# Fit the DT model to the training data.\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained DT model to make predictions on the test data.\n",
    "predictions = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluating metrics for the DT model.\n",
    "print(f\"DT Model's Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"DT Model's F1-Score: {f1_score(y_test, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    A random forest classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_trees : int, default=10\n",
    "        The number of trees in the random forest.\n",
    "    max_depth : int, default=2\n",
    "        The maximum depth of each decision tree in the random forest.\n",
    "    min_samples : int, default=7\n",
    "        The minimum number of samples required to split an internal node\n",
    "        of each decision tree in the random forest.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    n_trees : int\n",
    "        The number of trees in the random forest.\n",
    "    max_depth : int\n",
    "        The maximum depth of each decision tree in the random forest.\n",
    "    min_samples : int\n",
    "        The minimum number of samples required to split an internal node\n",
    "        of each decision tree in the random forest.\n",
    "    trees : list of DecisionTree\n",
    "        The decision trees in the random forest.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_trees=10, min_samples=7, max_depth=2):\n",
    "        \"\"\"\n",
    "        Initialize the random forest classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_trees : int, default=10\n",
    "            The number of trees in the random forest.\n",
    "        max_depth : int, default=2\n",
    "            The maximum depth of each decision tree in the random forest.\n",
    "        min_samples : int, default=7\n",
    "            The minimum number of samples required to split an internal node\n",
    "            of each decision tree in the random forest.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples = min_samples\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build a random forest classifier from the training set (X, y).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Create an empty list to store the trees.\n",
    "        self.trees = []\n",
    "        # Concatenate X and y into a single dataset.\n",
    "        dataset = np.concatenate((X, y), axis=1)\n",
    "        # Loop over the number of trees.\n",
    "        for _ in range(self.n_trees):\n",
    "            # Create a decision tree instance.\n",
    "            tree = DecisionTree(min_samples=self.min_samples, max_depth=self.max_depth)\n",
    "            # Sample from the dataset with replacement (bootstrapping).\n",
    "            dataset_sample = self._bootstrap_samples(dataset)\n",
    "            # Get the X and y samples from the dataset sample.\n",
    "            X_sample, y_sample = dataset_sample[:, :-1], dataset_sample[:, -1].reshape(-1, 1)\n",
    "            # Fit the tree to the X and y samples.\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            # Store the tree in the list of trees.\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def _bootstrap_samples(self, dataset):\n",
    "        \"\"\"\n",
    "        Bootstrap the dataset by sampling from it with replacement.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : array-like of shape (n_samples, n_features + 1)\n",
    "            The dataset to bootstrap.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dataset_sample : array-like of shape (n_samples, n_features + 1)\n",
    "            The bootstrapped dataset sample.\n",
    "        \"\"\"\n",
    "        # Get the number of samples in the dataset.\n",
    "        n_samples = dataset.shape[0]\n",
    "        # Generate random indices to index into the dataset with replacement.\n",
    "        np.random.seed(1)\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        # Return the bootstrapped dataset sample using the generated indices.\n",
    "        dataset_sample = dataset[indices]\n",
    "        return dataset_sample\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        \"\"\"\n",
    "        Return the most common label in an array of labels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array-like of shape (n_samples,)\n",
    "            The array of labels.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        most_occurring_value : int or float\n",
    "            The most common label in the array.\n",
    "        \"\"\"\n",
    "        y = list(y)\n",
    "        # Get the highest present class in the array\n",
    "        most_occurring_value = max(y, key=y.count)\n",
    "        return most_occurring_value\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        majority_predictions : array-like of shape (n_samples,)\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        # Get prediction from each tree in the tree list on the test data\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        # Get prediction for the same sample from all trees for each sample in the test data\n",
    "        preds = np.swapaxes(predictions, 0, 1)\n",
    "        # Get the most voted value by the trees and store it in the final predictions array\n",
    "        majority_predictions = np.array([self._most_common_label(pred) for pred in preds])\n",
    "        return majority_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Model's Accuracy: 0.9298245614035088\n",
      "RF Model's F1-Score: 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "# Create an RF model\n",
    "rf_model = RandomForest()\n",
    "\n",
    "# Fit the RF model to the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained RF model to make predictions on the test data\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluating metrics for the RF model\n",
    "print(f\"RF Model's Accuracy: {accuracy_score(y_test, rf_predictions)}\")\n",
    "print(f\"RF Model's F1-Score: {f1_score(y_test, rf_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost & XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've got familiar with the `AdaBoost` algorithm in the class and also through the slides. <br> Another famous algorithm between the ensemble methods is the `XGBoost`. You may have heard of it before, but to learn more about it, please refer to [this](https://en.wikipedia.org/wiki/XGBoost) link. It should be familiar to you, as you saw decision trees in the class.\n",
    "\n",
    "You can see the overview of how XGBoost works in the image below:\n",
    "\n",
    "<dev style=\"text-align: center\">\n",
    "<img src=\"./pics/XGBoost.png\" />\n",
    "</dev>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we're going to compare the accuracy of the three methods that we have mentioned above, `Random Forest`, `AdaBoost` and `XGBoost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AB Model's Accuracy: 0.9824561403508771\n",
      "AB Model's F1-Score: 0.9861111111111112\n",
      "XGB Model's Accuracy: 0.956140350877193\n",
      "XGB Model's F1-Score: 0.965034965034965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "ab_model = AdaBoostClassifier(n_estimators=10)\n",
    "xgb_model = XGBClassifier()\n",
    "\n",
    "ab_model.fit(X_train, y_train)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "ab_predictions = ab_model.predict(X_test)\n",
    "xgb_predictions = xgb_model.predict(X_test)\n",
    "\n",
    "print(f\"AB Model's Accuracy: {accuracy_score(y_test, ab_predictions)}\")\n",
    "print(f\"AB Model's F1-Score: {f1_score(y_test, ab_predictions)}\")\n",
    "\n",
    "print(f\"XGB Model's Accuracy: {accuracy_score(y_test, xgb_predictions)}\")\n",
    "print(f\"XGB Model's F1-Score: {f1_score(y_test, xgb_predictions)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
