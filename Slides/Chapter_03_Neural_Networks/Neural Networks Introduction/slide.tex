%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A Beamer template for University of Wollongong     %
% Based on THU beamer theme                          %
% Author: Qiuyu Lu                                   %
% Date: July 2024                                    %
% LPPL Licensed.                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Customized for Sharif University of Technology     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[serif, aspectratio=169]{beamer}
%\documentclass[serif]{beamer}  % for 4:3 ratio
\usepackage[T1]{fontenc} 
\usepackage{fourier} % see "http://faq.ktug.org/wiki/uploads/MathFonts.pdf" for other options
\usepackage{hyperref}
\usepackage{latexsym,amsmath,xcolor,multicol,booktabs,calligra}
\usepackage{graphicx,pstricks,listings,stackengine}
\usepackage{lipsum}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric} % Add this to include ellipse shapes
\usepackage{amsmath}
\usepackage{pgfplots}  % For plots
\usepackage{amsmath}   % For equations
\usepackage{array}     % For tables
\pgfplotsset{compat=1.16}

% \usetikzlibrary{positioning}


\author{Ali Sharifi-Zarchi}
\title{Machine Learning (CE 40717)}
\subtitle{Fall 2024}
\institute{
    CE Department \\
    Sharif University of Technology
}
%\date{\small \today}
% \usepackage{UoWstyle}
\usepackage{SUTstyle}

% defs
\def\cmd#1{\texttt{\color{red}\footnotesize $\backslash$#1}}
\def\env#1{\texttt{\color{blue}\footnotesize #1}}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{RGB}{153,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{halfgray}{gray}{0.55}

\lstset{
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{deepblue},
    emphstyle=\ttfamily\color{deepred},    % Custom highlighting style
    stringstyle=\color{deepgreen},
    numbers=left,
    numberstyle=\small\color{halfgray},
    rulesepcolor=\color{red!20!green!20!blue!20},
    frame=shadowbox,
}

\begin{document}

\begin{frame}
    \titlepage
    \vspace*{-0.6cm}
    \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/sharif-main-logo.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}    
\tableofcontents[sectionstyle=show,
subsectionstyle=show/shaded/hide,
subsubsectionstyle=show/shaded/hide]
\end{frame}

% ============================ Introduction ============================ 
\section{Introduction}

\begin{frame}{Perceptron Reminder}
    \begin{picture}(0,0)
            \put(200,-150){\includegraphics[width=7cm]{pic/1/neuron.png}} % Adjust path and size
    \end{picture}
    The building block of each neural network is Perceptron:
    \begin{itemize}
        \item $\{x_1, x_2, \dots, x_k\}$ : input features
        \item $\{w_1, w_2, \dots, w_k\}$ : feature weights
        \item $b$ : bias term
        \item $f(\cdot)$ : activation function
        \item $y$ : output of the neuron
    \end{itemize}
\end{frame}


\begin{frame}[t]{Perceptron Capacity}
\begin{figure}[h]
        \centering
        \begin{minipage}{0.45\textwidth}
            \begin{itemize}
                \item A perceptron solves linearly separable problems
                \item The XOR gate is not linearly separable
                \item We need a multi-layer perceptron for such problems
            \end{itemize}
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=\textwidth]{pic/1/xor.png}
            {\scriptsize XOR gate}
        \end{minipage}
    \end{figure}
\end{frame}


\begin{frame}{Why Neural Networks?}
    \begin{itemize}
        \item We can find explicit formulas for some problems (no machine learning)
        \begin{itemize}
            \item $\Delta x = \dfrac{1}{2}a\cdot t^2 + v_0 \cdot t$
        \end{itemize}
        \item We can model some problems assuming simple relationships (classical machine learning)
        \begin{itemize}
            \item House price as a linear function of its features
            \item $y = a_1 \cdot x_1 + a_2 \cdot x_2 + \ldots + a_p \cdot x_p$
        \end{itemize}
        \item How about classifying these images?
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.1]{pic/1/fashion-mnist.png}
        \end{center}
    \end{itemize}
\end{frame}


\begin{frame}[t]{Why Neural Networks? Cont.}
    \begin{picture}(0,0)
            \put(200, -150){\includegraphics[width=7cm]{pic/1/sneakers.png}}
    \end{picture}
    
    \begin{itemize}
        \item No explicit formula exists to recognize a sneaker
        \item We recognize any sneaker intuitively
        \item Our brains use a complex function for this recognition
        \item \textbf{Deep neural networks} can learn this complex function
    \end{itemize}
\end{frame}

%
\section{Multi-Layer Perceptron (MLP)}

\begin{frame}{Example: MLP for Complex Patterns}
    \begin{itemize}
        \item What network to learn this area?
        \item Example is adapted from [1].
        \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/2/ex1.png}
        \end{center}
    \end{figure}
    \end{itemize}
\end{frame}


\begin{frame}{Example: MLP for Complex Patterns Cont.}
        \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/2/ex2.png}
        \end{center}
    \end{figure}
\end{frame}


\begin{frame}{Example: MLP for Complex Patterns Cont.}
        \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/2/ex3.png}
        \end{center}
    \end{figure}
\end{frame}


\begin{frame}{Example: MLP for Complex Patterns Cont.}
        \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/2/ex4.png}
        \end{center}
    \end{figure}
\end{frame}


\begin{frame}{Example: MLP for Complex Patterns Cont.}
        \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/2/ex5.png}
        \end{center}
    \end{figure}
\end{frame}


\begin{frame}{Example: MLP for Complex Patterns Cont.}
        \begin{figure}[htpb]
        \begin{center}
            \includegraphics[keepaspectratio, scale=0.25]{pic/2/ex6.png}
        \end{center}
    \end{figure}
\end{frame}

\begin{frame}{MLP Capacity}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Increasing width and depth allows us to approximate complex decision boundaries
                \item An activation function makes a neuronâ€™s output non-linear, allowing the network to learn complex data
                \item Not limited to Boolean or step functions
                \item With appropriate activation functions, neural networks can approximate any real-valued function (More details later)
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \includegraphics[width=\textwidth]{pic/2/activations.png} \\
            \begin{center}
            {\scriptsize Adapted from \href{https://sefiks.com/2020/02/02/dance-moves-of-deep-learning-activation-functions/}{Sefiks}}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}


\section{Neural Networks}
\begin{frame}{Single Hidden Layer Neural Network}
    \minipage{.5\textwidth}
    \begin{itemize}
        \item Hidden layer pre-activation:
        $$a(x)_i = b^{(1)}_i + \sum_j W^{(1)}_{ij} \cdot x_j$$
        \item Activated hidden layer:
        $$h(x) = f(a(x))$$
        \item Output layer:
        $$ o(x) = o\left(b^{(2)} + W^{(2)}h^{(1)}x \right) $$
    \end{itemize}
    \endminipage
    \hfill
    \minipage{.48\textwidth}
        \begin{figure}[bh]
            \includegraphics[width=\linewidth]{pic/2/single-hidden_nn.png}
        \end{figure}
    \endminipage
\end{frame}


\begin{frame}{Multi-Hidden Layer Neural Network}
    \minipage{.4\textwidth}
    \begin{itemize}
        \item Let $h^0_i = x_i$ for $i \in \{1, 2, \ldots, n  \}$
        \item For $\ell \in \{0, 1, \ldots, L \}$:
        $$a^{(\ell+1)}_j = b^{(\ell)}_j + \sum_i W^{(\ell)}_{ij}\cdot h^{(\ell)}_i$$
        $$h^{(\ell+1)}_j = f(a^{(\ell+1)}_j)$$

        \item Learnable parameters:
        $$b^{(\ell)}_j, W^{(\ell)}_{ij}$$

        \item Number of learnable parameters:
        $$(n+1)m_1 + (m_1 + 1)m_2 + ... + (m_L + 1)k$$
    \end{itemize}
    \endminipage
    \hfill
    \minipage{.6\textwidth}
        \begin{figure}[bh]
            \includegraphics[width=\linewidth]{pic/2/multi-hidden-nn.png}
        \end{figure}
    \endminipage
\end{frame}

\begin{frame}[t]{Deep Neural Network Architecture}
\begin{itemize}
    \item More than a few hidden layers: Deep Neural Network (DNN)
    \item Designing a neural network architecture is more of an art than a science.
        \begin{figure}[bh]
            \includegraphics[keepaspectratio, scale=0.3]{pic/3/huge-nn.png}
        \end{figure}
\end{itemize}
\end{frame}

\begin{frame}[t]{Network Width and Depth}
    \begin{columns}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item \textbf{Width:} More neurons, more complexity
                \item \textbf{Depth:} More layers, more abstraction
                \item \textbf{Balance:} 
                \begin{itemize}
                    \item Too narrow/shallow: risk of underfitting
                    \item Too wide/deep: risk of overfitting
                \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
                \includegraphics[keepaspectratio, width=\textwidth]{pic/3/complexity.png} \\
                {\scriptsize Adapted from \href{https://towardsdatascience.com/overfitting-and-underfitting-principles-ea8964d9c45c}{Towards Data Science}}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\section{Training Neural Networks}
\begin{frame}{Training Phases}
    \begin{itemize}
        \item \textbf{Initialize weights and biases}: These values control how the network initially processes information (More details later)
        \item \textbf{Forward pass}: Pass the input through the network to get an output
        \item \textbf{Calculate the error}: Compare the network's output to the correct answer to measure the difference (called the 'loss' - More details later) 
        \item \textbf{Backpropagation}: Use the loss value to adjust the weights and biases to improve the network's accuracy
    \end{itemize}
    \begin{figure}[bh]
            \includegraphics[keepaspectratio, scale=0.2]{pic/4/training-phases.png}
    \end{figure}
\end{frame}

\begin{frame}[t]{Forward Propagation}
    \begin{itemize}
        \item This is the pass where we send input data through the network to make a prediction (likely inaccurate at first).
        \item The prediction is made by calculating weighted sums and applying an activation function at each layer
        $$ o = a^{(L)} = f^{(L)}\left( b^{(L)} + W^{(L)} \cdot f^{(L-1)}\left( \ldots f^{(1)}(b^{(1)} + W^{(1)x})  \ldots \right) \right) $$
    \end{itemize}
\end{frame}

\begin{frame}{Forward Propagation Cont.}
    \minipage{.45\textwidth}
        \begin{figure}[bh]
            \includegraphics[width=\linewidth]{pic/4/before-forward.png}
            {\scriptsize Before producing predictions. Adapted from \href{https://mlu-explain.github.io/neural-networks/}{mlu-explain}.}
        \end{figure}
    \endminipage
    \hfill
    \minipage{.45\textwidth}
        \begin{figure}[bh]
            \includegraphics[width=\linewidth]{pic/4/after-forward.png}
            {\scriptsize After producing predictions. Adapted from \href{https://mlu-explain.github.io/neural-networks/}{mlu-explain}.}
        \end{figure}
    \endminipage
\end{frame}


\begin{frame}[t]{Forward Propagation Cont.}
    \begin{itemize}
        \item The goal is to adjust the networkâ€™s parameters to improve the predictions
        \item The loss is calculated after the forward pass, telling us how far off our predictions are from the true values
        \begin{figure}[bh]
            \centering
            \includegraphics[keepaspectratio, scale=0.25]{pic/4/loss-values.png} \\
            {\scriptsize Loss values for predictions. Adapted from \href{https://mlu-explain.github.io/neural-networks/}{mlu-explain}.}
        \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}[t]{BackPropagation and Parameter Update}
    \begin{itemize}
        \item The network uses the \textbf{loss} to adjust its \textbf{weights and biases} through a process called \textbf{backpropagation}
        \item Backpropagation calculates how much weights should change to reduce the error
        \item This will be explained in more detail in the next lecture
        \begin{figure}[bh]
            \centering
            \includegraphics[keepaspectratio, scale=0.25]{pic/4/after-backprop.png} \\
            {\scriptsize Predictions get better as the weights get updated. Adapted from \href{https://mlu-explain.github.io/neural-networks/}{mlu-explain}.}
        \end{figure}
    \end{itemize}
\end{frame}


\section{References}

% \begin{frame}[allowframebreaks]
% \bibliography{ref}
%    \bibliographystyle{ieeetr}
%     \nocite{*} % used here because no citation happens in slides
%    % if there are too many try useï¼š
%     % \tiny\bibliographystyle{alpha}
%  \end{frame}

\end{document}